{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdYnGYHITNtC"
      },
      "source": [
        "### 구글 드라이브 연동"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha4_mpIZTNtF",
        "outputId": "d772fd71-5817-44f3-d029-fb6a99bd55b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9fExALdTNtF",
        "outputId": "02ca19ae-8804-4717-fedb-ba823e152be6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/oss_drive\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/oss_drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_gb74v7TNtG"
      },
      "source": [
        "### imports 및 seed fix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKliaR1kUJsm",
        "outputId": "887d0adb-51f0-4174-b842-94d2ccf3da93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchbearer\n",
            "  Downloading torchbearer-0.5.3-py3-none-any.whl (138 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/138.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m102.4/138.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.1/138.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from torchbearer) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchbearer) (1.23.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchbearer) (4.66.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchbearer) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchbearer) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchbearer) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchbearer) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchbearer) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchbearer) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->torchbearer) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->torchbearer) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->torchbearer) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->torchbearer) (1.3.0)\n",
            "Installing collected packages: torchbearer\n",
            "Successfully installed torchbearer-0.5.3\n",
            "Collecting pkbar\n",
            "  Downloading pkbar-0.5-py3-none-any.whl (9.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pkbar) (1.23.5)\n",
            "Installing collected packages: pkbar\n",
            "Successfully installed pkbar-0.5\n"
          ]
        }
      ],
      "source": [
        "# 세션 만료시마다 실행필요\n",
        "%pip install torchbearer\n",
        "%pip install pkbar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNiCiGmFTNtG"
      },
      "outputs": [],
      "source": [
        "# 필요한 imports\n",
        "import torchvision\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchbearer\n",
        "from torchbearer import Trial\n",
        "import pkbar\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import datetime\n",
        "import configparser\n",
        "# image file truncated error prevention\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSB64UQATNtG",
        "outputId": "8bfd6ff1-e20e-47fc-82df-4abc544a9a67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79910ef61bd0>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# seed fix\n",
        "SEED = 3\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIwdZ-qpTNtH"
      },
      "source": [
        "### 데이터 로더 생성 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLXkuqxwTNtH"
      },
      "outputs": [],
      "source": [
        "class PyTorchData():\n",
        "    def __init__(self, _dataType, config):\n",
        "        '''\n",
        "        input_dim : image size\n",
        "        data_path : training data path\n",
        "        batch_size : batch size\n",
        "        '''\n",
        "\n",
        "        if _dataType == \"data\":\n",
        "            self.m_DataDim = int(config['input_dim'])\n",
        "        elif _dataType == \"image\":\n",
        "            self.m_ImageDim = int(config['input_dim'])\n",
        "            self.m_DataPath = config['data_path']\n",
        "            self.m_BatchSize = int(config['batch_size'])\n",
        "\n",
        "\n",
        "    # training할 이미지 로더 생성 및 리턴\n",
        "    def ImageTrain(self):\n",
        "        transDatagen = transforms.Compose([transforms.Resize((self.m_ImageDim, self.m_ImageDim)),\n",
        "                                           transforms.ToTensor()])\n",
        "\n",
        "        trainPath = self.m_DataPath + '/training'\n",
        "        trainFolder = torchvision.datasets.ImageFolder(root = trainPath,\n",
        "                                                       transform = transDatagen) # 폴더 정리가 같은 클래스 별로 이미 데이터 정리되어있음\n",
        "\n",
        "        trainLoader = DataLoader(trainFolder,\n",
        "                                batch_size = self.m_BatchSize,\n",
        "                                shuffle = True)\n",
        "\n",
        "        print(\"Train Class [\", trainLoader.dataset.class_to_idx, \"]\")\n",
        "        print(\"Train Numbers [\", len(trainLoader.dataset.imgs), \"]\")\n",
        "        print(\"Train Batch Size [\", trainLoader.batch_size, \"]\")\n",
        "\n",
        "        return trainLoader\n",
        "\n",
        "    # validation할 이미지 로더 생성 및 리턴\n",
        "    def ImageValidation(self):\n",
        "        transDatagen = transforms.Compose([transforms.Resize((self.m_ImageDim, self.m_ImageDim)),\n",
        "                                           transforms.ToTensor()])\n",
        "\n",
        "        validationPath = self.m_DataPath + '/validation'\n",
        "        validationSet = torchvision.datasets.ImageFolder(root = validationPath,\n",
        "                                                         transform = transDatagen) # 폴더 정리가 같은 클래스 별로 이미 데이터 정리되어있음\n",
        "\n",
        "        validationLoader = DataLoader(validationSet,\n",
        "                                      batch_size = self.m_BatchSize,\n",
        "                                      shuffle = False)\n",
        "\n",
        "        print(\"Validation Class [\", validationLoader.dataset.class_to_idx, \"]\")\n",
        "        print(\"Validation Numbers [\", len(validationLoader.dataset.imgs),\"]\")\n",
        "        print(\"Validation Batch Size [\", validationLoader.batch_size,\"]\")\n",
        "\n",
        "        return validationLoader\n",
        "\n",
        "    # testing할 이미지 로더 생성 및 리턴\n",
        "    def ImageTest(self):\n",
        "        transDatagen = transforms.Compose([transforms.Resize((self.m_ImageDim, self.m_ImageDim)),\n",
        "                                           transforms.ToTensor()])\n",
        "\n",
        "        testDirectory = self.m_DataPath + '/testing'\n",
        "        testSet = torchvision.datasets.ImageFolder(root = testDirectory,\n",
        "                                                   transform = transDatagen) # 폴더 정리가 같은 클래스 별로 이미 데이터 정리되어있음\n",
        "\n",
        "        testLoader = DataLoader(testSet,\n",
        "                                batch_size = self.m_BatchSize,\n",
        "                                shuffle = False)\n",
        "\n",
        "        print(\"Test Class [\", testLoader.dataset.class_to_idx, \"]\")\n",
        "        print(\"Test Numbers [\", len(testLoader.dataset.imgs), \"]\")\n",
        "        print(\"Test Batch Size [\", testLoader.batch_size,\"]\")\n",
        "\n",
        "        return testLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKKjFJT3TNtI"
      },
      "source": [
        "### 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agTcEzGWd8K4"
      },
      "outputs": [],
      "source": [
        "# 모델 구조 정의\n",
        "class PillModel(nn.Module):\n",
        "\n",
        "    # bulid cnn model\n",
        "    def __init__(self, config):\n",
        "        super(PillModel, self).__init__()\n",
        "        '''\n",
        "        ClassNum : class number\n",
        "        '''\n",
        "        self.m_ClassNum = int(config['class_num'])\n",
        "\n",
        "        channel1 = 16\n",
        "        channel2 = 32\n",
        "        channel3 = 64\n",
        "        conv1Size = 3\n",
        "        conv2Size = 3\n",
        "        poolSize = 2\n",
        "\n",
        "        self.m_Conv1 = nn.Conv2d(in_channels = 3, out_channels = channel1, kernel_size = conv1Size, padding = 1)\n",
        "        self.m_Pool1 = nn.MaxPool2d(poolSize, poolSize)\n",
        "        self.m_Conv2 = nn.Conv2d(in_channels = channel1, out_channels = channel2, kernel_size = conv2Size, padding = 1)\n",
        "        self.m_Pool2 = nn.MaxPool2d(poolSize, poolSize)\n",
        "        self.m_Conv3 = nn.Conv2d(in_channels = channel2, out_channels = channel3, kernel_size = conv2Size, padding = 1)\n",
        "        self.m_Pool3 = nn.MaxPool2d(poolSize, poolSize)\n",
        "\n",
        "        self.m_Linear4 = nn.Linear(40000, 256)\n",
        "        self.m_Drop4 = nn.Dropout2d(0.5)\n",
        "\n",
        "        self.m_Linear5 = nn.Linear(256, self.m_ClassNum)\n",
        "        self.m_Relu = nn.ReLU()\n",
        "\n",
        "    # forward 연산 정의\n",
        "    def forward(self, x):\n",
        "        x = self.m_Relu(self.m_Conv1(x))\n",
        "        x = self.m_Pool1(x)\n",
        "\n",
        "        x = self.m_Relu(self.m_Conv2(x))\n",
        "        x = self.m_Pool2(x)\n",
        "\n",
        "        x = self.m_Relu(self.m_Conv3(x))\n",
        "        x = self.m_Pool3(x)\n",
        "\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = self.m_Relu(self.m_Linear4(x))\n",
        "        x = self.m_Drop4(x)\n",
        "\n",
        "        x = self.m_Linear5(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC1YzdowTNtJ"
      },
      "source": [
        "### 모델 만들기\n",
        "모델 저장, 학습, 테스팅 작업 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG1vrhDcePlp"
      },
      "outputs": [],
      "source": [
        "# 모델 hyperparam설정, 저장, 학습, 테스팅 주요 기능\n",
        "class MakeModel():\n",
        "\n",
        "    def __init__(self,config):\n",
        "        '''\n",
        "        learning_rate : learning rate\n",
        "        epochs : epoch\n",
        "        save_path : save path\n",
        "        model_name : model save name\n",
        "        '''\n",
        "        self._epoch = int(config['epochs'])\n",
        "        self._lr = float(config['learning_rate'])\n",
        "        self._savePath = config['save_path']\n",
        "        self._modelName = config['model_name']\n",
        "\n",
        "    # 모델.pt 저장\n",
        "    def SaveModel(self, _model, optimizer, _trainData, trainLoss):\n",
        "        nowdate = datetime.datetime.now().strftime('%y%m%d_%H')\n",
        "        ret = 0\n",
        "        try:\n",
        "            torch.save({'model_state_dict': _model.state_dict(),\n",
        "                        'epoch': self._epoch,\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                        'loss': trainLoss,\n",
        "                        'label_name':_trainData.dataset.classes},\n",
        "                        self._savePath + '/' + self._modelName + '_PyTorchModel.pt')\n",
        "            print(\"model saved [\", self._savePath + '/' + self._modelName + \"_PyTorchModel.pt ]\")\n",
        "\n",
        "        except PermissionError:\n",
        "                torch.save(_model, './' + nowdate + '_' + self._modelName + '_PyTorchModel.pt')\n",
        "                print('model saved [ ./' + nowdate + '_' + self._modelName + '_PyTorchModel.pt ]')\n",
        "\n",
        "        except IOError as e:\n",
        "            print(\"IOError except: \", e.errno)\n",
        "            ret = 1\n",
        "\n",
        "        return ret\n",
        "\n",
        "    # 모델 학습\n",
        "    def Training(self, _device, _model, _trainData, _valData):\n",
        "        _model.train()\n",
        "\n",
        "        optimizer = optim.Adam(_model.parameters(), lr = self._lr)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        bestValLoss = float('inf')\n",
        "\n",
        "        for epoch in range(self._epoch):\n",
        "            trainLoss = 0.0\n",
        "            trainSize = 0.0\n",
        "            trainCorrect = 0.0\n",
        "\n",
        "            print(\"Epoch {}/{}\".format(epoch + 1, self._epoch))\n",
        "            progress = pkbar.Kbar(target=len(_trainData), width = 25)\n",
        "\n",
        "            # train\n",
        "            for batchIdx, data in enumerate(_trainData):\n",
        "                images, labels = data\n",
        "                images, labels = images.to(_device), labels.to(_device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = _model(images)\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                trainLoss = loss.item()\n",
        "\n",
        "                _, predicted = outputs.max(1)\n",
        "                trainSize += labels.shape[0]\n",
        "                trainCorrect += predicted.eq(labels.view_as(predicted)).sum().item()\n",
        "                trainAccuracy = 100 * trainCorrect / trainSize\n",
        "\n",
        "                progress.update(batchIdx, values = [(\"loss: \", trainLoss), (\"acc: \", trainAccuracy)])\n",
        "\n",
        "                del loss\n",
        "                del outputs\n",
        "\n",
        "            # validation\n",
        "            with torch.no_grad():\n",
        "                valLoss = 0.0\n",
        "                valSize = 0.0\n",
        "                valCorrect = 0.0\n",
        "\n",
        "                for batchIdx, data in enumerate(_valData):\n",
        "                    images, labels = data\n",
        "                    images, labels = images.to(_device), labels.to(_device)\n",
        "\n",
        "                    outputs = _model(images)\n",
        "                    valLoss = criterion(outputs, labels).item()\n",
        "\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    valSize += labels.shape[0]\n",
        "\n",
        "                    valCorrect += predicted.eq(labels.view_as(predicted)).sum().item()\n",
        "                    valAccuracy = 100 * valCorrect / valSize\n",
        "\n",
        "                progress.add(1, values=[(\"val loss\", valLoss), (\"val acc\", valAccuracy)])\n",
        "\n",
        "            # if best loss value, save model\n",
        "            if valLoss < bestValLoss:\n",
        "                bestValLoss = valLoss\n",
        "                ret = self.SaveModel(_model, optimizer, _trainData, trainLoss)\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "    # 테스트 데이터로 모델 테스팅 (한 번만 진행되어야 함)\n",
        "    def Testing(self, _device, _model, _testData):\n",
        "        _model.eval()\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        testLoss = 0.0\n",
        "        testSize = 0.0\n",
        "        testCorrect = 0.0\n",
        "\n",
        "        progress = pkbar.Kbar(target=len(_testData), width = 25)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batchIdx, data in enumerate(_testData):\n",
        "                images, labels = data\n",
        "                images, labels = images.to(_device), labels.to(_device)\n",
        "                outputs = _model(images)\n",
        "\n",
        "                testLoss = criterion(outputs, labels).item()\n",
        "\n",
        "                _, predicted = outputs.data.max(1)\n",
        "                testSize += labels.shape[0]\n",
        "                testCorrect += predicted.eq(labels.view_as(predicted)).sum().item()\n",
        "                accuracy = 100 * testCorrect / testSize\n",
        "\n",
        "                progress.update(batchIdx, values = [(\"test loss: \", testLoss), (\"test acc: \", accuracy)])\n",
        "\n",
        "            testLoss /= len(_testData.dataset)\n",
        "        progress.add(1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IK763TkTNtJ"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28AuG8nTeYSi"
      },
      "outputs": [],
      "source": [
        "# 해당 파일 실행 시 실행되는 main\n",
        "class PyTorchMain():\n",
        "\n",
        "    def __init__(self):\n",
        "        config = configparser.ConfigParser()\n",
        "        config.read('./Modelings/modeling_config.ini', encoding='UTF-8')\n",
        "\n",
        "        self.m_Device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.m_cPytorchModel = PillModel(config['PT_model_info'])\n",
        "\n",
        "        self.m_cPytorchData = PyTorchData(\"image\", config['PT_model_info'])\n",
        "\n",
        "        self.m_cMakeModel = MakeModel(config['PT_model_info'])\n",
        "\n",
        "\n",
        "    def main(self):\n",
        "        print(\"\\n[ Model ]\\n\")\n",
        "        model = self.m_cPytorchModel.to(self.m_Device)\n",
        "        print(model)\n",
        "\n",
        "        # load dataset\n",
        "        print(\"\\n[ Data ]\\n\")\n",
        "        trainData = self.m_cPytorchData.ImageTrain()\n",
        "        valData = self.m_cPytorchData.ImageValidation()\n",
        "        testData = self.m_cPytorchData.ImageTest()\n",
        "\n",
        "        # training\n",
        "        print(\"\\n[ Training ]\\n\")\n",
        "        ret = self.m_cMakeModel.Training(_device = self.m_Device,\n",
        "                                         _model = model,\n",
        "                                         _trainData = trainData,\n",
        "                                         _valData = valData)\n",
        "        if ret == 0 or ret == 1:\n",
        "            # testing\n",
        "            print(\"\\n[ Testing ]\\n\")\n",
        "            self.m_cMakeModel.Testing(_device = self.m_Device,\n",
        "                                      _model = model,\n",
        "                                      _testData = testData)\n",
        "\n",
        "        '''\n",
        "        class_to_idx json 파일로 출력.\n",
        "        f = open(\"C:\\oss_medi\\WhatIsMethIs-Model\\model\\Modelings\\model\\trainData_class_to_idx_230828.json\", 'w')\n",
        "        f.write(json.dumps(trainData.dataset.class_to_idx))\n",
        "        f.close()\n",
        "        '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qdOlZ9hTNtJ",
        "outputId": "edb0a167-0d37-416c-ba0a-ac1f1bbdb1a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modeling.py 실행시작\n",
            "\n",
            "[ Model ]\n",
            "\n",
            "PillModel(\n",
            "  (m_Conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (m_Pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (m_Conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (m_Pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (m_Conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (m_Pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (m_Linear4): Linear(in_features=40000, out_features=256, bias=True)\n",
            "  (m_Drop4): Dropout2d(p=0.5, inplace=False)\n",
            "  (m_Linear5): Linear(in_features=256, out_features=200, bias=True)\n",
            "  (m_Relu): ReLU()\n",
            ")\n",
            "\n",
            "[ Data ]\n",
            "\n",
            "Train Class [ {'29002': 0, '34342': 1, '37990': 2, '39916': 3, '40122': 4, '40720': 5, '40767': 6, '40792': 7, '40837': 8, '40949': 9, '40953': 10, '40990': 11, '40991': 12, '41097': 13, '41107': 14, '41169': 15, '41170': 16, '41172': 17, '41207': 18, '41225': 19, '41327': 20, '41344': 21} ]\n",
            "Train Numbers [ 2244 ]\n",
            "Train Batch Size [ 16 ]\n",
            "Validation Class [ {'29002': 0, '34342': 1, '37990': 2, '39916': 3, '40122': 4, '40720': 5, '40767': 6, '40792': 7, '40837': 8, '40949': 9, '40953': 10, '40990': 11, '40991': 12, '41097': 13, '41107': 14, '41169': 15, '41170': 16, '41172': 17, '41207': 18, '41225': 19, '41327': 20, '41344': 21} ]\n",
            "Validation Numbers [ 616 ]\n",
            "Validation Batch Size [ 16 ]\n",
            "Test Class [ {'29002': 0, '34342': 1, '37990': 2, '39916': 3, '40122': 4, '40720': 5, '40767': 6, '40792': 7, '40837': 8, '40949': 9, '40953': 10, '40990': 11, '40991': 12, '41097': 13, '41107': 14, '41169': 15, '41170': 16, '41172': 17, '41207': 18, '41225': 19, '41327': 20, '41344': 21} ]\n",
            "Test Numbers [ 308 ]\n",
            "Test Batch Size [ 16 ]\n",
            "\n",
            "[ Training ]\n",
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "141/141 [=========================] - 2505s 18s/step - loss: : 2.4920 - acc: : 15.2130 - val loss: 1.7872 - val acc: 57.7922\n",
            "model saved [ ./Modelings/model/230828_model_01_PyTorchModel.pt ]\n",
            "Epoch 2/10\n",
            "141/141 [=========================] - 55s 392ms/step - loss: : 0.7952 - acc: : 65.7255 - val loss: 0.5197 - val acc: 77.4351\n",
            "model saved [ ./Modelings/model/230828_model_01_PyTorchModel.pt ]\n",
            "Epoch 3/10\n",
            "141/141 [=========================] - 55s 392ms/step - loss: : 0.4269 - acc: : 82.9164 - val loss: 0.7432 - val acc: 81.9805\n",
            "Epoch 4/10\n",
            "141/141 [=========================] - 55s 388ms/step - loss: : 0.2965 - acc: : 87.4791 - val loss: 0.0759 - val acc: 92.2078\n",
            "model saved [ ./Modelings/model/230828_model_01_PyTorchModel.pt ]\n",
            "Epoch 5/10\n",
            "141/141 [=========================] - 56s 398ms/step - loss: : 0.2089 - acc: : 92.0262 - val loss: 0.2154 - val acc: 91.7208\n",
            "Epoch 6/10\n",
            "141/141 [=========================] - 55s 388ms/step - loss: : 0.1325 - acc: : 95.4101 - val loss: 0.0417 - val acc: 94.8052\n",
            "model saved [ ./Modelings/model/230828_model_01_PyTorchModel.pt ]\n",
            "Epoch 7/10\n",
            "141/141 [=========================] - 55s 391ms/step - loss: : 0.1183 - acc: : 95.4231 - val loss: 0.1394 - val acc: 94.3182\n",
            "Epoch 8/10\n",
            "141/141 [=========================] - 55s 388ms/step - loss: : 0.1218 - acc: : 95.9562 - val loss: 0.0060 - val acc: 94.9675\n",
            "model saved [ ./Modelings/model/230828_model_01_PyTorchModel.pt ]\n",
            "Epoch 9/10\n",
            "141/141 [=========================] - 55s 390ms/step - loss: : 0.0985 - acc: : 96.2886 - val loss: 0.0718 - val acc: 96.2662\n",
            "Epoch 10/10\n",
            "141/141 [=========================] - 54s 386ms/step - loss: : 0.0642 - acc: : 97.0712 - val loss: 0.0576 - val acc: 95.2922\n",
            "\n",
            "[ Testing ]\n",
            "\n",
            "20/20 [=========================] - 280s 14s/step - test loss: : 0.0503 - test acc: : 97.9674\n",
            "####### Modeling.py 실행 finish #######\n"
          ]
        }
      ],
      "source": [
        "# 해당 파일이 모듈로서 말고 직접 실행될 때\n",
        "# ex) python ./Modelings/Modeling.py [config파일명 있어도 되고 없어도됨]\n",
        "if __name__ == '__main__':\n",
        "    print(\"Modeling.py 실행시작\")\n",
        "    mainClass = PyTorchMain()\n",
        "    mainClass.main()\n",
        "    print('####### Modeling.py 실행 finish #######')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
